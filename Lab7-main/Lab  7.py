import re
import bz2
import os
import math
import urllib.request
from collections import defaultdict, Counter
from nltk.util import ngrams

# === 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ –∫–æ—Ä–ø—É—Å—É ===
def download_and_extract_corpus(url, output_file):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–∞ —Ä–æ–∑–ø–∞–∫–æ–≤—É—î bz2 –∫–æ—Ä–ø—É—Å, —è–∫—â–æ –π–æ–≥–æ —â–µ –Ω–µ–º–∞—î"""
    compressed_file = output_file + ".bz2"
    if not os.path.exists(output_file):
        print("‚¨á –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ—Ä–ø—É—Å—É UberCorpus...")
        urllib.request.urlretrieve(url, compressed_file)

        print(" –†–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è –∫–æ—Ä–ø—É—Å—É...")
        with bz2.open(compressed_file, "rt", encoding="utf-8") as f_in, open(output_file, "w", encoding="utf-8") as f_out:
            for line in f_in:
                f_out.write(line)
        print("–ö–æ—Ä–ø—É—Å –≥–æ—Ç–æ–≤–∏–π:", output_file)
    else:
        print("–ö–æ—Ä–ø—É—Å —É–∂–µ —Ä–æ–∑–ø–∞–∫–æ–≤–∞–Ω–∏–π:", output_file)


def load_corpus(file_path, max_tokens=200_000):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –∫–æ—Ä–ø—É—Å—É"""
    tokens = []
    with open(file_path, encoding="utf-8") as f:
        for line in f:
            tokens.extend(re.findall(r"\w+", line.lower()))
            if len(tokens) >= max_tokens:
                break
    print(f" –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(tokens)} —Ç–æ–∫–µ–Ω—ñ–≤")
    return tokens[:max_tokens]


# === 2. –ü–æ–±—É–¥–æ–≤–∞ N-–≥—Ä–∞–º –º–æ–¥–µ–ª—ñ ===
def build_ngram_model(tokens, n):
    model = defaultdict(Counter)
    for gram in ngrams(tokens, n):
        prefix, word = tuple(gram[:-1]), gram[-1]
        model[prefix][word] += 1
    return model


# === 3. –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ª–æ–≤–∞ ===
def predict_next(model, context, top_k=5):
    context = tuple(context[-(len(next(iter(model))) if model else 0):])
    candidates = model.get(context, {})
    total = sum(candidates.values())
    probs = {word: count / total for word, count in candidates.items()} if total > 0 else {}
    return sorted(probs.items(), key=lambda x: x[1], reverse=True)[:top_k]


# === 4. –ê–≤—Ç–æ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É ===
def autocomplete(text, models, top_k=5):
    tokens = text.lower().split()
    for n in reversed(range(2, 6)):
        if len(tokens) >= n - 1:
            context = tokens[-(n - 1):]
            if tuple(context) in models[n]:
                return predict_next(models[n], context, top_k)
    total = sum(models[1].values())
    probs = {word: count / total for word, count in models[1].items()}
    return sorted(probs.items(), key=lambda x: x[1], reverse=True)[:top_k]


# === 5. –ü–µ—Ä–ø–ª–µ–∫—Å—ñ—è (–æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ) ===
def perplexity(model, tokens, n):
    N = 0
    log_prob = 0
    for gram in ngrams(tokens, n):
        prefix, word = tuple(gram[:-1]), gram[-1]
        prefix_count = sum(model[prefix].values())
        word_count = model[prefix][word]
        prob = word_count / prefix_count if prefix_count > 0 else 1e-6
        log_prob += math.log(prob)
        N += 1
    return math.exp(-log_prob / N)


# === 6. –û—Å–Ω–æ–≤–Ω–∏–π –∑–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    url = "https://lang.org.ua/static/downloads/corpora/ubercorpus.tokenized.shuffled.txt.bz2"
    file_path = "ubercorpus.tokenized.txt"

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ —Ä–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è –∫–æ—Ä–ø—É—Å—É
    download_and_extract_corpus(url, file_path)

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤
    tokens = load_corpus(file_path, max_tokens=200_000)

    # –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª–µ–π N-–≥—Ä–∞–º
    print("üîß –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª–µ–π N-–≥—Ä–∞–º...")
    models = {}
    for n in range(1, 6):
        if n == 1:
            models[n] = Counter(tokens)
        else:
            models[n] = build_ngram_model(tokens, n)

    # –¢–µ—Å—Ç –∞–≤—Ç–æ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    print("\n –¢–µ—Å—Ç –∞–≤—Ç–æ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è:")
    test_input = "—É–∫—Ä–∞—ó–Ω–∞ —î"
    predictions = autocomplete(test_input, models)
    print(f"–í–≤–µ–¥–µ–Ω–Ω—è: '{test_input}'")
    for i, (word, prob) in enumerate(predictions):
        print(f"{i+1}. {word} (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å: {prob:.4f})")

    # –û—Ü—ñ–Ω–∫–∞ –ø–µ—Ä–ø–ª–µ–∫—Å—ñ—ó
    print("\n—ã –û—Ü—ñ–Ω–∫–∞ –ø–µ—Ä–ø–ª–µ–∫—Å—ñ—ó (–Ω–∞ 3-–≥—Ä–∞–º—ñ):")
    pp = perplexity(models[3], tokens[:10000], 3)
    print(f"–ü–µ—Ä–ø–ª–µ–∫—Å—ñ—è: {pp:.2f}")
